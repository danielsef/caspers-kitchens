{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "this notebook creates a complaint handling agent with tools to investigate orders and make decisions about credits, investigations, or escalations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": "%%writefilev agent.py\nimport json\nfrom typing import Annotated, Any, Generator, Optional, Sequence, TypedDict, Union, Literal, cast\nfrom uuid import uuid4\n\nimport mlflow\nfrom databricks_langchain import (\n    ChatDatabricks,\n    DatabricksFunctionClient,\n    UCFunctionToolkit,\n    set_uc_function_client,\n)\nfrom langchain_core.language_models import LanguageModelLike\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n)\nfrom langchain_core.messages.utils import convert_to_messages\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\nfrom langchain_core.tools import BaseTool\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt.tool_node import ToolNode\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n    ResponsesAgentStreamEvent,\n)\n\nmlflow.langchain.autolog()\n\nLLM_MODEL = \"{{LLM_MODEL}}\"\nCATALOG = \"{{CATALOG}}\"\n\nclient = DatabricksFunctionClient()\nset_uc_function_client(client)\n\nclass ComplaintResponse(TypedDict):\n    order_id: str\n    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n    decision: Literal[\"auto_credit\", \"investigate\", \"escalate\"]\n    credit_amount: float\n    rationale: str\n    customer_response: str\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    custom_inputs: Optional[dict[str, Any]]\n    custom_outputs: Optional[dict[str, Any]]\n\nLLM_ENDPOINT_NAME = f\"{LLM_MODEL}\"\nbase_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n\nsystem_prompt = \"\"\"You are ComplaintAgent for Casper's ghost kitchen.\n\nProcess:\n1. Extract order_id from complaint\n2. Call get_order_overview(order_id)\n3. Call get_order_timing(order_id)\n4. If timing complaint, call get_location_timings(location)\n5. Make decision\n\nDecisions:\n- AUTO-CREDIT: Clear minor issues (late >P75)\n- INVESTIGATE: Moderate issues (food quality, missing items)\n- ESCALATE: Severe issues (legal, safety)\n\nReturn JSON with: order_id, complaint_category, decision, credit_amount, rationale, customer_response\"\"\"\n\ntools: list[BaseTool] = []\nuc_tool_names = [\n    f\"{CATALOG}.ai.get_order_overview\",\n    f\"{CATALOG}.ai.get_order_timing\",\n    f\"{CATALOG}.ai.get_location_timings\",\n]\nuc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\ntools.extend(uc_toolkit.tools)\n\nRESPONSE_FIELDS = {\n    \"order_id\",\n    \"complaint_category\",\n    \"decision\",\n    \"credit_amount\",\n    \"rationale\",\n    \"customer_response\",\n}\n\ndef parse_structured_response(obj: Union[AIMessage, dict[str, Any]]) -> ComplaintResponse:\n    \"\"\"Coerce an AIMessage or dict into the ComplaintResponse schema.\"\"\"\n    if isinstance(obj, dict):\n        candidate = obj\n    else:\n        parsed = obj.additional_kwargs.get(\"parsed_structured_output\")\n        if isinstance(parsed, dict):\n            candidate = parsed\n        else:\n            content = obj.content\n            if isinstance(content, str):\n                raw = content\n            elif isinstance(content, list):\n                raw = \"\".join(part.get(\"text\", \"\") if isinstance(part, dict) else str(part) for part in content)\n            else:\n                raise ValueError(\"Unsupported message content type for structured output\")\n            candidate = json.loads(raw)\n\n    missing = RESPONSE_FIELDS.difference(candidate.keys())\n    if missing:\n        raise ValueError(f\"Structured response missing fields: {sorted(missing)}\")\n\n    return cast(ComplaintResponse, candidate)\n\n\ndef create_tool_calling_agent(\n    model: LanguageModelLike,\n    tools: Union[ToolNode, Sequence[BaseTool]],\n    system_prompt: Optional[str] = None,\n):\n    tool_model = model.bind_tools(tools, tool_choice=\"auto\")\n    structured_model = tool_model.with_structured_output(ComplaintResponse)\n\n    def should_continue(state: AgentState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n            return \"continue\"\n        return \"end\"\n\n    if system_prompt:\n        preprocessor = RunnableLambda(\n            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n        )\n    else:\n        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n\n    tool_runnable = preprocessor | tool_model\n    structured_runnable = preprocessor | structured_model\n\n    def call_model(state: AgentState, config: RunnableConfig):\n        response = tool_runnable.invoke(state, config)\n        if not isinstance(response, AIMessage):\n            raise ValueError(f\"Expected AIMessage from model, received {type(response)}\")\n\n        if response.tool_calls:\n            return {\"messages\": [response]}\n\n        try:\n            parsed = parse_structured_response(response)\n        except (json.JSONDecodeError, ValueError):\n            structured = structured_runnable.invoke(state, config)\n            parsed = parse_structured_response(structured)\n\n        structured_message = AIMessage(\n            id=response.id or str(uuid4()),\n            content=json.dumps(parsed),\n            additional_kwargs={\"parsed_structured_output\": parsed},\n        )\n        return {\"messages\": [structured_message]}\n\n    workflow = StateGraph(AgentState)\n    workflow.add_node(\"agent\", RunnableLambda(call_model))\n    workflow.add_node(\"tools\", ToolNode(tools))\n    workflow.set_entry_point(\"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n        {\"continue\": \"tools\", \"end\": END},\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n    return workflow.compile()\n\n\nclass LangGraphResponsesAgent(ResponsesAgent):\n    def __init__(self, agent):\n        self.agent = agent\n\n    def _langchain_to_responses(self, messages: list[BaseMessage]) -> list[dict[str, Any]]:\n        \"\"\"Convert LangChain messages to Responses output items.\"\"\"\n        items: list[dict[str, Any]] = []\n\n        for raw_message in messages or []:\n            message = raw_message.model_dump() if hasattr(raw_message, \"model_dump\") else raw_message\n            role = message.get(\"type\")\n\n            if role == \"ai\":\n                if tool_calls := message.get(\"tool_calls\"):\n                    for tool_call in tool_calls:\n                        items.append(\n                            self.create_function_call_item(\n                                id=message.get(\"id\") or str(uuid4()),\n                                call_id=tool_call[\"id\"],\n                                name=tool_call[\"name\"],\n                                arguments=json.dumps(tool_call.get(\"args\", {})),\n                            )\n                        )\n                    continue\n\n                content = message.get(\"content\")\n                if isinstance(content, list):\n                    text_content = \"\".join(ch.get(\"text\", \"\") if isinstance(ch, dict) else str(ch) for ch in content)\n                elif isinstance(content, str):\n                    text_content = content\n                else:\n                    text_content = json.dumps(content)\n\n                items.append(\n                    self.create_text_output_item(\n                        text=text_content,\n                        id=message.get(\"id\") or str(uuid4()),\n                    )\n                )\n            elif role == \"tool\":\n                items.append(\n                    self.create_function_call_output_item(\n                        call_id=message.get(\"tool_call_id\"),\n                        output=message.get(\"content\"),\n                    )\n                )\n\n        return items\n\n    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n        outputs = [\n            event.item\n            for event in self.predict_stream(request)\n            if event.type == \"response.output_item.done\"\n        ]\n        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n\n    def predict_stream(\n        self,\n        request: ResponsesAgentRequest,\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        lc_msgs = convert_to_messages(self.prep_msgs_for_cc_llm(request.input))\n\n        for event in self.agent.stream({\"messages\": lc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n            if event[0] == \"updates\":\n                for node_data in event[1].values():\n                    for item in self._langchain_to_responses(node_data.get(\"messages\", [])):\n                        yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n            elif event[0] == \"messages\":\n                chunk = event[1][0]\n                if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n                    yield ResponsesAgentStreamEvent(\n                        **self.create_text_delta(delta=content, item_id=chunk.id),\n                    )\n\nagent = create_tool_calling_agent(base_llm, tools, system_prompt)\nAGENT = LangGraphResponsesAgent(agent)\nmlflow.models.set_model(AGENT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": "import mlflow\nfrom agent import LLM_ENDPOINT_NAME, tools\nfrom mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\nfrom pkg_resources import get_distribution\n\nresources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\nfor tool in tools:\n    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n\n# ResponsesAgentRequest format uses \"input\" not \"messages\"\ninput_example = {\n    \"input\": [\n        {\n            \"role\": \"user\",\n            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n        }\n    ]\n}\n\nwith mlflow.start_run():\n    logged_agent_info = mlflow.pyfunc.log_model(\n        name=\"complaint_agent\",\n        python_model=\"agent.py\",\n        input_example=input_example,\n        resources=resources,\n        pip_requirements=[\n            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n            f\"mlflow=={get_distribution('mlflow').version}\",\n            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n            f\"langgraph=={get_distribution('langgraph').version}\",\n        ],\n    )\n\nmlflow.set_active_model(model_id = logged_agent_info.model_id)"
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": [
    "#### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive complaint scenarios for evaluation\nimport random\n\n# Get sample order IDs for different scenarios\nall_order_ids = [\n    row['order_id'] for row in spark.sql(f\"\"\"\n        SELECT DISTINCT order_id \n        FROM {CATALOG}.lakeflow.all_events \n        WHERE event_type='delivered'\n        LIMIT 30\n    \"\"\").collect()\n]\n\n# Create diverse complaint scenarios\ncomplaint_scenarios = []\n\n# Delivery delay complaints (should get AUTO-CREDIT if truly >P75)\nfor oid in all_order_ids[:5]:\n    complaint_scenarios.extend([\n        f\"My order took forever to arrive! Order ID: {oid}\",\n        f\"Order was 2 hours late, unacceptable. ID: {oid}\",\n    ])\n\n# Food quality complaints (should be INVESTIGATE, not auto-credit)\nfor oid in all_order_ids[5:10]:\n    complaint_scenarios.extend([\n        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n    ])\n\n# Missing items complaints (should be INVESTIGATE)\nfor oid in all_order_ids[10:13]:\n    complaint_scenarios.extend([\n        f\"Half my order was missing - no drinks or sides! Order: {oid}\",\n        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n    ])\n\n# Service issues (should be INVESTIGATE)\nfor oid in all_order_ids[13:15]:\n    complaint_scenarios.extend([\n        f\"Your driver was extremely rude to me. Order: {oid}\",\n        f\"Driver left my food in the wrong building. Order: {oid}\",\n    ])\n\n# Escalation triggers (should be ESCALATE)\nfor oid in all_order_ids[15:17]:\n    complaint_scenarios.extend([\n        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n        f\"This food poisoning could have killed me! Order: {oid}\",\n    ])\n\n# Sample for reasonable eval size\ncomplaint_scenarios = random.sample(complaint_scenarios, min(15, len(complaint_scenarios)))\n\n# Wrap in correct input schema for ResponsesAgent\ndata = []\nfor complaint in complaint_scenarios:\n    data.append({\n        \"inputs\": {\n            \"input\": [{\n                \"role\": \"user\",\n                \"content\": complaint\n            }]\n        }\n    })\n\nprint(f\"Created {len(data)} evaluation scenarios\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": "# Create multiple scorers and run evaluation\n\nfrom mlflow.genai.scorers import Guidelines\nimport mlflow\nimport sys\nimport os\nsys.path.append(os.getcwd())\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nproject_directory = os.path.dirname(notebook_path)\nsys.path.append(project_directory)\n\nfrom agent import AGENT\n\n# Multiple scorers to evaluate different aspects\nrefund_reason = Guidelines(\n    name=\"refund_reason\",\n    guidelines=[\n        \"If a refund is offered, it must clearly relate to the complaint made by the user\",\n        \"Do not offer timing-related refunds for food quality or missing item complaints\"\n    ]\n)\n\ndecision_quality = Guidelines(\n    name=\"decision_quality\",\n    guidelines=[\n        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Service complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n    ]\n)\n\nevidence_usage = Guidelines(\n    name=\"evidence_usage\",\n    guidelines=[\n        \"Decisions must be based on actual order data from tool calls\",\n        \"Credit amounts should be justified by delivery time comparisons to percentiles\",\n        \"Do not make assumptions without checking order details\"\n    ]\n)\n\n# ResponsesAgent predict function wrapper for evaluation\n# Note: parameter name must match the key in eval data (\"input\")\ndef predict_fn(input):\n    from mlflow.types.responses import ResponsesAgentRequest\n    request = ResponsesAgentRequest(input=input)\n    response = AGENT.predict(request)\n    return response.output[0].text\n\n# Run evaluation with multiple scorers\nresults = mlflow.genai.evaluate(\n    data=data,\n    scorers=[refund_reason, decision_quality, evidence_usage],\n    predict_fn=predict_fn\n)\n\nprint(f\"Evaluation complete. Check MLflow UI for detailed results.\")"
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": [
    "#### log complaint agent to `UC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": [
    "#### deploy the agent to model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME, \n",
    "    model_version=uc_registered_model_info.version, \n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=f\"{dbutils.widgets.get('COMPLAINT_AGENT_ENDPOINT_NAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": [
    "##### record model in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": [
    "#### production monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}