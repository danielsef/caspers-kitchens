{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "this notebook creates a complaint handling agent with tools to investigate orders and make decisions about credits, investigations, or escalations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "id": "tdy6yy3gheg",
   "source": "%sql\nCREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\nWITH order_times AS (\n  SELECT\n    order_id,\n    location,\n    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n  FROM\n    ${CATALOG}.lakeflow.all_events\n  WHERE\n    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n  GROUP BY\n    order_id,\n    location\n),\ntotal_order_times AS (\n  SELECT\n    order_id,\n    location,\n    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n  FROM\n    order_times\n  WHERE\n    order_created_time IS NOT NULL\n    AND delivered_time IS NOT NULL\n)\nSELECT\n  location,\n  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n  PERCENTILE(total_order_time_minutes, 0.99) AS P99\nFROM\n  total_order_times\nGROUP BY\n  location",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "source": "#### Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bd3tu3r2gso",
   "source": "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\ndbutils.library.restartPython()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dl04kgwv9ap",
   "source": "CATALOG = dbutils.widgets.get(\"CATALOG\")\nLLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bruu85upqq5",
   "source": "import re\nfrom IPython.core.magic import register_cell_magic\n\n@register_cell_magic\ndef writefilev(line, cell):\n    \"\"\"\n    %%writefilev file.py\n    Allows {{var}} substitutions while leaving normal {} intact.\n    \"\"\"\n    filename = line.strip()\n\n    def replacer(match):\n        expr = match.group(1)\n        return str(eval(expr, globals(), locals()))\n\n    # Replace only double braces {{var}}\n    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n\n    with open(filename, \"w\") as f:\n        f.write(content)\n    print(f\"Wrote file with substitutions: {filename}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w2c70l4ca8",
   "source": "%%writefilev agent.py\nfrom typing import Any, Generator, Optional, Sequence, Union\n\nimport mlflow\nfrom databricks_langchain import (\n    ChatDatabricks,\n    VectorSearchRetrieverTool,\n    DatabricksFunctionClient,\n    UCFunctionToolkit,\n    set_uc_function_client,\n)\nfrom langchain_core.language_models import LanguageModelLike\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\nfrom langchain_core.tools import BaseTool\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt.tool_node import ToolNode\nfrom mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\nfrom mlflow.pyfunc import ChatAgent\nfrom mlflow.types.agent import (\n    ChatAgentChunk,\n    ChatAgentMessage,\n    ChatAgentResponse,\n    ChatContext,\n)\n\nmlflow.langchain.autolog()\n\nclient = DatabricksFunctionClient()\nset_uc_function_client(client)\n\n############################################\n# Define your LLM endpoint and system prompt\n############################################\nLLM_ENDPOINT_NAME = f\"{{LLM_MODEL}}\"\nllm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n\nsystem_prompt = \"\"\"You are ComplaintAgent, a unified customer service agent for Chef Casper's multi-brand ghost kitchen operation.\n\nYou handle customer complaints by investigating orders and making data-driven decisions about credits, investigations, or escalations.\n\nProcess:\n1. Extract order_id from the customer complaint\n2. Call `get_order_overview(order_id)` to get basic order details\n3. Call `get_order_timing(order_id)` to get delivery timeline\n4. If timing-related complaint, call `get_location_timings(location)` for context\n5. Classify the complaint and make a decision\n\nDecision Framework:\n- AUTO-CREDIT: Clear, minor issues (late delivery >P75, missing low-value items)\n- INVESTIGATE: Uncertain or moderate issues (food quality claims, service complaints, billing)\n- ESCALATE: Severe issues (safety concerns, threats, high-value claims)\n\nBe helpful but data-driven. Only offer credits when justified by evidence.\n\nAlways provide:\n- order_id (the order identifier from the complaint)\n- complaint_category (delivery_delay, missing_items, food_quality, service_issue, billing, other)\n- decision (auto_credit, investigate, escalate)\n- credit_amount (if applicable)\n- rationale (clear explanation based on order data)\n- customer_response (professional response text)\n\nOutput a single-line JSON with these fields. No extra text or markdown.\"\"\"\n\n###############################################################################\n## Define tools for your agent\n###############################################################################\ntools = []\n\nuc_tool_names = [f\"{{CATALOG}}.ai.get_order_overview\", \n                 f\"{{CATALOG}}.ai.get_order_timing\",\n                 f\"{{CATALOG}}.ai.get_location_timings\"]\nuc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\ntools.extend(uc_toolkit.tools)\n\n#####################\n## Define agent logic\n#####################\n\n\ndef create_tool_calling_agent(\n    model: LanguageModelLike,\n    tools: Union[Sequence[BaseTool], ToolNode],\n    system_prompt: Optional[str] = None,\n) -> CompiledGraph:\n    model = model.bind_tools(tools)\n\n    # Define the function that determines which node to go to\n    def should_continue(state: ChatAgentState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # If there are function calls, continue. else, end\n        if last_message.get(\"tool_calls\"):\n            return \"continue\"\n        else:\n            return \"end\"\n\n    if system_prompt:\n        preprocessor = RunnableLambda(\n            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n            + state[\"messages\"]\n        )\n    else:\n        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n    model_runnable = preprocessor | model\n\n    def call_model(\n        state: ChatAgentState,\n        config: RunnableConfig,\n    ):\n        response = model_runnable.invoke(state, config)\n\n        return {\"messages\": [response]}\n\n    workflow = StateGraph(ChatAgentState)\n\n    workflow.add_node(\"agent\", RunnableLambda(call_model))\n    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n\n    workflow.set_entry_point(\"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n        {\n            \"continue\": \"tools\",\n            \"end\": END,\n        },\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    return workflow.compile()\n\n\nclass LangGraphChatAgent(ChatAgent):\n    def __init__(self, agent: CompiledStateGraph):\n        self.agent = agent\n\n    def predict(\n        self,\n        messages: list[ChatAgentMessage],\n        context: Optional[ChatContext] = None,\n        custom_inputs: Optional[dict[str, Any]] = None,\n    ) -> ChatAgentResponse:\n        request = {\"messages\": self._convert_messages_to_dict(messages)}\n\n        messages = []\n        for event in self.agent.stream(request, stream_mode=\"updates\"):\n            for node_data in event.values():\n                messages.extend(\n                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n                )\n        return ChatAgentResponse(messages=messages)\n\n    def predict_stream(\n        self,\n        messages: list[ChatAgentMessage],\n        context: Optional[ChatContext] = None,\n        custom_inputs: Optional[dict[str, Any]] = None,\n    ) -> Generator[ChatAgentChunk, None, None]:\n        request = {\"messages\": self._convert_messages_to_dict(messages)}\n        for event in self.agent.stream(request, stream_mode=\"updates\"):\n            for node_data in event.values():\n                yield from (\n                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n                )\n\n\n# Create the agent object, and specify it as the agent object to use when\n# loading the agent back for inference via mlflow.models.set_model()\nagent = create_tool_calling_agent(llm, tools, system_prompt)\nAGENT = LangGraphChatAgent(agent)\nmlflow.models.set_model(AGENT)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dd6gjrp4mx6",
   "source": "# get an actual order_id for input example\nsample_order_id = spark.sql(f\"\"\"\n    SELECT order_id \n    FROM {CATALOG}.lakeflow.all_events \n    WHERE event_type='delivered'\n    LIMIT 1\n\"\"\").collect()[0]['order_id']",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ih3tt5qeb5",
   "source": "assert sample_order_id is not None\nprint(sample_order_id)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "23o4h7j6amzh",
   "source": "import mlflow\nfrom agent import LLM_ENDPOINT_NAME, tools\nfrom databricks_langchain import VectorSearchRetrieverTool\nfrom mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\nfrom pkg_resources import get_distribution\nfrom unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n\nresources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\nfor tool in tools:\n    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n        }\n    ]\n}\n\nwith mlflow.start_run():\n    logged_agent_info = mlflow.pyfunc.log_model(\n        name=\"complaint_agent\",\n        python_model=\"agent.py\",\n        input_example=input_example,\n        resources=resources,\n        pip_requirements=[\n            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n            f\"mlflow=={get_distribution('mlflow').version}\",\n            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n            f\"langgraph=={get_distribution('langgraph').version}\",\n        ],\n    )\n\nmlflow.set_active_model(model_id = logged_agent_info.model_id)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "source": "#### eval",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2loovjto96g",
   "source": "# sample 10 order_ids\ncomplaint_queries = [\n    row['order_id'] for row in spark.sql(f\"\"\"\n        SELECT order_id \n        FROM {CATALOG}.lakeflow.all_events \n        WHERE event_type='delivered'\n        LIMIT 10\n    \"\"\").collect()\n]\n\n# wrap in correct input schema with complaint text\ndata = []\nfor query in complaint_queries:\n    data.append(\n        {\n            \"inputs\": {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"My order was late and the food quality was poor! Order: {query}\",\n                    }\n                ]\n            },\n        }\n    )\n\nprint(data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9l6zrp5n03b",
   "source": "# create guideline, run evals\n\nfrom mlflow.genai.scorers import Guidelines\nimport mlflow\nimport sys\nimport os\nsys.path.append(os.getcwd())\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nproject_directory = os.path.dirname(notebook_path)\n\n# Add the project directory to the system path\nsys.path.append(project_directory)\n\nfrom agent import AGENT\n\ndecision_quality = Guidelines(\n    name=\"decision_quality\",\n    guidelines=[\n        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Service complaints should be classified as 'investigate', not 'auto_credit'\",\n        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n    ]\n)\n\nresults = mlflow.genai.evaluate(\n    data=data,\n    scorers=[decision_quality],\n    predict_fn = lambda messages: AGENT.predict({\"messages\": messages})\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "source": "#### log complaint agent to `UC`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y8lfco9zzn",
   "source": "mlflow.set_registry_uri(\"databricks-uc\")\n\nUC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n\n# register the model to UC\nuc_registered_model_info = mlflow.register_model(\n    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "source": "#### deploy the agent to model serving",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "exckljo2zx4",
   "source": "from databricks import agents\ndeployment_info = agents.deploy(\n    model_name=UC_MODEL_NAME, \n    model_version=uc_registered_model_info.version, \n    scale_to_zero=False,\n    endpoint_name=f\"{dbutils.widgets.get('COMPLAINT_AGENT_ENDPOINT_NAME')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i1syfkwcivl",
   "source": "print(deployment_info)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "source": "##### record model in state",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ihnhnv5plw",
   "source": "# Also add to UC-state\nimport sys\nsys.path.append('../utils')\nfrom uc_state import add\n\nadd(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}