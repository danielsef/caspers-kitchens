{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "#### complaint agent\n",
    "\n",
    "this notebook creates a complaint handling agent with tools to investigate orders and make decisions about credits, investigations, or escalations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Tool & View Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ${CATALOG}.ai;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdy6yy3gheg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW ${CATALOG}.ai.order_delivery_times_per_location_view AS\n",
    "WITH order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    MAX(CASE WHEN event_type = 'order_created' THEN try_to_timestamp(ts) END) AS order_created_time,\n",
    "    MAX(CASE WHEN event_type = 'delivered' THEN try_to_timestamp(ts) END) AS delivered_time\n",
    "  FROM\n",
    "    ${CATALOG}.lakeflow.all_events\n",
    "  WHERE\n",
    "    try_to_timestamp(ts) >= CURRENT_TIMESTAMP() - INTERVAL 1 DAY\n",
    "  GROUP BY\n",
    "    order_id,\n",
    "    location\n",
    "),\n",
    "total_order_times AS (\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    (UNIX_TIMESTAMP(delivered_time) - UNIX_TIMESTAMP(order_created_time)) / 60 AS total_order_time_minutes\n",
    "  FROM\n",
    "    order_times\n",
    "  WHERE\n",
    "    order_created_time IS NOT NULL\n",
    "    AND delivered_time IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  location,\n",
    "  PERCENTILE(total_order_time_minutes, 0.50) AS P50,\n",
    "  PERCENTILE(total_order_time_minutes, 0.75) AS P75,\n",
    "  PERCENTILE(total_order_time_minutes, 0.99) AS P99\n",
    "FROM\n",
    "  total_order_times\n",
    "GROUP BY\n",
    "  location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_overview(oid STRING COMMENT 'The unique order identifier to retrieve information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  location STRING COMMENT 'Order location',\n",
    "  items_json STRING COMMENT 'JSON array of ordered items with details',\n",
    "  customer_address STRING COMMENT 'Customer delivery address',\n",
    "  brand_id BIGINT COMMENT 'Brand ID for the order',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created'\n",
    ")\n",
    "COMMENT 'Returns basic order information including items, location, and customer details'\n",
    "RETURN\n",
    "  WITH order_created_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      location,\n",
    "      get_json_object(body, '$.items') as items_json,\n",
    "      get_json_object(body, '$.customer_addr') as customer_address,\n",
    "      -- Extract brand_id from first item in the order\n",
    "      CAST(get_json_object(get_json_object(body, '$.items[0]'), '$.brand_id') AS BIGINT) as brand_id,\n",
    "      try_to_timestamp(ts) as order_created_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid AND event_type = 'order_created'\n",
    "    LIMIT 1\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    location,\n",
    "    items_json,\n",
    "    customer_address,\n",
    "    brand_id,\n",
    "    order_created_ts\n",
    "  FROM order_created_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_order_timing(oid STRING COMMENT 'The unique order identifier to get timing information for')\n",
    "RETURNS TABLE (\n",
    "  order_id STRING COMMENT 'The order id',\n",
    "  order_created_ts TIMESTAMP COMMENT 'When the order was created',\n",
    "  delivered_ts TIMESTAMP COMMENT 'When the order was delivered (NULL if not delivered)',\n",
    "  delivery_duration_minutes FLOAT COMMENT 'Time from order creation to delivery in minutes (NULL if not delivered)',\n",
    "  delivery_status STRING COMMENT 'Current delivery status: delivered, in_progress, or unknown'\n",
    ")\n",
    "COMMENT 'Returns timing information for a specific order'\n",
    "RETURN\n",
    "  WITH order_events AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      event_type,\n",
    "      try_to_timestamp(ts) as event_ts\n",
    "    FROM ${CATALOG}.lakeflow.all_events\n",
    "    WHERE order_id = oid\n",
    "  ),\n",
    "  timing_summary AS (\n",
    "    SELECT\n",
    "      order_id,\n",
    "      MIN(CASE WHEN event_type = 'order_created' THEN event_ts END) as order_created_ts,\n",
    "      MAX(CASE WHEN event_type = 'delivered' THEN event_ts END) as delivered_ts\n",
    "    FROM order_events\n",
    "    GROUP BY order_id\n",
    "  )\n",
    "  SELECT\n",
    "    order_id,\n",
    "    order_created_ts,\n",
    "    delivered_ts,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL AND order_created_ts IS NOT NULL THEN\n",
    "        CAST((UNIX_TIMESTAMP(delivered_ts) - UNIX_TIMESTAMP(order_created_ts)) / 60 AS FLOAT)\n",
    "      ELSE NULL\n",
    "    END as delivery_duration_minutes,\n",
    "    CASE\n",
    "      WHEN delivered_ts IS NOT NULL THEN 'delivered'\n",
    "      WHEN order_created_ts IS NOT NULL THEN 'in_progress'\n",
    "      ELSE 'unknown'\n",
    "    END as delivery_status\n",
    "  FROM timing_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION ${CATALOG}.ai.get_location_timings(loc STRING COMMENT 'Location name as a string')\n",
    "RETURNS TABLE (\n",
    "  location STRING COMMENT 'Location of the order source',\n",
    "  P50 FLOAT COMMENT '50th percentile delivery time in minutes',\n",
    "  P75 FLOAT COMMENT '75th percentile delivery time in minutes',\n",
    "  P99 FLOAT COMMENT '99th percentile delivery time in minutes'\n",
    ")\n",
    "COMMENT 'Returns the 50/75/99th percentile of delivery times for a location to benchmark order timing'\n",
    "RETURN\n",
    "  SELECT location, P50, P75, P99\n",
    "  FROM ${CATALOG}.ai.order_delivery_times_per_location_view AS odlt\n",
    "  WHERE odlt.location = loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq5ml4wp6v",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3tu3r2gso",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow-skinny[databricks] langgraph==0.3.4 databricks-langchain databricks-agents uv\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dl04kgwv9ap",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "LLM_MODEL = dbutils.widgets.get(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bruu85upqq5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writefilev(line, cell):\n",
    "    \"\"\"\n",
    "    %%writefilev file.py\n",
    "    Allows {{var}} substitutions while leaving normal {} intact.\n",
    "    \"\"\"\n",
    "    filename = line.strip()\n",
    "\n",
    "    def replacer(match):\n",
    "        expr = match.group(1)\n",
    "        return str(eval(expr, globals(), locals()))\n",
    "\n",
    "    # Replace only double braces {{var}}\n",
    "    content = re.sub(r\"\\{\\{(.*?)\\}\\}\", replacer, cell)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Wrote file with substitutions: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2c70l4ca8",
   "metadata": {},
   "outputs": [],
   "source": "%%writefilev agent.py\nfrom typing import Any, Generator, Optional, Sequence, Union, Literal, TypedDict\nimport json\n\nimport mlflow\nfrom databricks_langchain import (\n    ChatDatabricks,\n    VectorSearchRetrieverTool,\n    DatabricksFunctionClient,\n    UCFunctionToolkit,\n    set_uc_function_client,\n)\nfrom langchain_core.language_models import LanguageModelLike\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\nfrom langchain_core.tools import BaseTool\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt.tool_node import ToolNode\nfrom mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\nfrom mlflow.pyfunc import ResponsesAgent\nfrom mlflow.types.responses import (\n    ResponsesAgentRequest,\n    ResponsesAgentResponse,\n    ResponsesAgentStreamEvent,\n)\n\nmlflow.langchain.autolog()\n\nclient = DatabricksFunctionClient()\nset_uc_function_client(client)\n\n############################################\n# Define structured output schema\n############################################\nclass ComplaintResponse(TypedDict):\n    \"\"\"Structured response for complaint handling\"\"\"\n    order_id: str\n    complaint_category: Literal[\"delivery_delay\", \"missing_items\", \"food_quality\", \"service_issue\", \"billing\", \"other\"]\n    decision: Literal[\"auto_credit\", \"investigate\", \"escalate\"]\n    credit_amount: float\n    rationale: str\n    customer_response: str\n\n############################################\n# Define your LLM endpoint and system prompt\n############################################\nLLM_ENDPOINT_NAME = f\"{{LLM_MODEL}}\"\nbase_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n\nsystem_prompt = \"\"\"You are ComplaintAgent, a unified customer service agent for Chef Casper's multi-brand ghost kitchen operation.\n\nYou handle customer complaints by investigating orders and making data-driven decisions about credits, investigations, or escalations.\n\nProcess:\n1. Extract order_id from the customer complaint\n2. Call `get_order_overview(order_id)` to get basic order details\n3. Call `get_order_timing(order_id)` to get delivery timeline\n4. If timing-related complaint, call `get_location_timings(location)` for context\n5. Classify the complaint and make a decision\n\nDecision Framework:\n- AUTO-CREDIT: Clear, minor issues (late delivery >P75, missing low-value items)\n- INVESTIGATE: Uncertain or moderate issues (food quality claims, service complaints, billing)\n- ESCALATE: Severe issues (safety concerns, threats, high-value claims)\n\nBe helpful but data-driven. Only offer credits when justified by evidence.\n\nReturn your response as JSON matching this schema:\n{\n  \"order_id\": \"string\",\n  \"complaint_category\": \"delivery_delay|missing_items|food_quality|service_issue|billing|other\",\n  \"decision\": \"auto_credit|investigate|escalate\",\n  \"credit_amount\": float (0 if no credit),\n  \"rationale\": \"string explaining decision based on data\",\n  \"customer_response\": \"string with professional customer message\"\n}\"\"\"\n\n###############################################################################\n## Define tools for your agent\n###############################################################################\ntools = []\n\nuc_tool_names = [f\"{{CATALOG}}.ai.get_order_overview\", \n                 f\"{{CATALOG}}.ai.get_order_timing\",\n                 f\"{{CATALOG}}.ai.get_location_timings\"]\nuc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\ntools.extend(uc_toolkit.tools)\n\n# IMPORTANT: Bind tools FIRST, then structured output\nllm = base_llm.bind_tools(tools).with_structured_output(ComplaintResponse)\n\n#####################\n## Define agent logic\n#####################\n\n\ndef create_tool_calling_agent(\n    model: LanguageModelLike,\n    tools: Union[Sequence[BaseTool], ToolNode],\n    system_prompt: Optional[str] = None,\n) -> CompiledGraph:\n    # Model already has tools bound\n    \n    # Define the function that determines which node to go to\n    def should_continue(state: ChatAgentState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # Handle both dict messages (with tool_calls) and structured output (dict)\n        if isinstance(last_message, dict):\n            # Check for tool calls in dict format\n            if last_message.get(\"tool_calls\"):\n                return \"continue\"\n        # If no tool calls, we're done\n        return \"end\"\n\n    if system_prompt:\n        preprocessor = RunnableLambda(\n            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n            + state[\"messages\"]\n        )\n    else:\n        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n    model_runnable = preprocessor | model\n\n    def call_model(\n        state: ChatAgentState,\n        config: RunnableConfig,\n    ):\n        response = model_runnable.invoke(state, config)\n\n        return {\"messages\": [response]}\n\n    workflow = StateGraph(ChatAgentState)\n\n    workflow.add_node(\"agent\", RunnableLambda(call_model))\n    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n\n    workflow.set_entry_point(\"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n        {\n            \"continue\": \"tools\",\n            \"end\": END,\n        },\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    return workflow.compile()\n\n\nclass LangGraphResponsesAgent(ResponsesAgent):\n    def __init__(self, agent: CompiledStateGraph):\n        self.agent = agent\n\n    def predict(\n        self,\n        request: ResponsesAgentRequest,\n    ) -> ResponsesAgentResponse:\n        # Convert input messages to LangGraph format\n        messages = [{\"role\": msg.role, \"content\": msg.content} for msg in request.input]\n        \n        # Execute the agent\n        agent_messages = []\n        for event in self.agent.stream({\"messages\": messages}, stream_mode=\"updates\"):\n            for node_data in event.values():\n                agent_messages.extend(node_data.get(\"messages\", []))\n        \n        # Get the final response (structured output from LLM)\n        final_message = agent_messages[-1]\n        \n        # If it's a dict (TypedDict output), serialize it\n        if isinstance(final_message, dict):\n            # Check if it's a ComplaintResponse or a regular message\n            if \"content\" in final_message:\n                response_text = final_message[\"content\"]\n            else:\n                # It's the structured output\n                response_text = json.dumps(final_message, indent=2)\n        else:\n            response_text = str(final_message)\n        \n        # Return ResponsesAgentResponse with text output\n        return ResponsesAgentResponse(\n            output=[\n                self.create_text_output_item(\n                    text=response_text,\n                    id=\"complaint_response\"\n                )\n            ]\n        )\n\n    def predict_stream(\n        self,\n        request: ResponsesAgentRequest,\n    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n        # Convert input messages to LangGraph format\n        messages = [{\"role\": msg.role, \"content\": msg.content} for msg in request.input]\n        \n        # Stream agent execution\n        for event in self.agent.stream({\"messages\": messages}, stream_mode=\"updates\"):\n            for node_data in event.values():\n                for msg in node_data.get(\"messages\", []):\n                    # Serialize message content\n                    if isinstance(msg, dict):\n                        if \"content\" in msg:\n                            content = msg[\"content\"]\n                        else:\n                            content = json.dumps(msg, indent=2)\n                    else:\n                        content = str(msg)\n                    \n                    # Yield text delta\n                    yield self.create_text_delta(\n                        delta=content,\n                        item_id=\"complaint_response\"\n                    )\n\n\n# Create the agent object, and specify it as the agent object to use when\n# loading the agent back for inference via mlflow.models.set_model()\nagent = create_tool_calling_agent(llm, tools, system_prompt)\nAGENT = LangGraphResponsesAgent(agent)\nmlflow.models.set_model(AGENT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6gjrp4mx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an actual order_id for input example\n",
    "sample_order_id = spark.sql(f\"\"\"\n",
    "    SELECT order_id \n",
    "    FROM {CATALOG}.lakeflow.all_events \n",
    "    WHERE event_type='delivered'\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]['order_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ih3tt5qeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_order_id is not None\n",
    "print(sample_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23o4h7j6amzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME, tools\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in tools:\n",
    "    resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "# ResponsesAgentRequest format uses \"input\" not \"messages\"\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"My order was really late! Order ID: {sample_order_id}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"complaint_agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "mlflow.set_active_model(model_id = logged_agent_info.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfympowd7q",
   "metadata": {},
   "source": [
    "#### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2loovjto96g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive complaint scenarios for evaluation\n",
    "import random\n",
    "\n",
    "# Get sample order IDs for different scenarios\n",
    "all_order_ids = [\n",
    "    row['order_id'] for row in spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT order_id \n",
    "        FROM {CATALOG}.lakeflow.all_events \n",
    "        WHERE event_type='delivered'\n",
    "        LIMIT 30\n",
    "    \"\"\").collect()\n",
    "]\n",
    "\n",
    "# Create diverse complaint scenarios\n",
    "complaint_scenarios = []\n",
    "\n",
    "# Delivery delay complaints (should get AUTO-CREDIT if truly >P75)\n",
    "for oid in all_order_ids[:5]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My order took forever to arrive! Order ID: {oid}\",\n",
    "        f\"Order was 2 hours late, unacceptable. ID: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Food quality complaints (should be INVESTIGATE, not auto-credit)\n",
    "for oid in all_order_ids[5:10]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"My falafel was completely soggy and inedible. Order: {oid}\",\n",
    "        f\"The food was cold when it arrived, very disappointing. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Missing items complaints (should be INVESTIGATE)\n",
    "for oid in all_order_ids[10:13]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Half my order was missing - no drinks or sides! Order: {oid}\",\n",
    "        f\"My entire falafel bowl was missing from the order! Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Service issues (should be INVESTIGATE)\n",
    "for oid in all_order_ids[13:15]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"Your driver was extremely rude to me. Order: {oid}\",\n",
    "        f\"Driver left my food in the wrong building. Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Escalation triggers (should be ESCALATE)\n",
    "for oid in all_order_ids[15:17]:\n",
    "    complaint_scenarios.extend([\n",
    "        f\"I'm calling my lawyer about this terrible service! Order: {oid}\",\n",
    "        f\"This food poisoning could have killed me! Order: {oid}\",\n",
    "    ])\n",
    "\n",
    "# Sample for reasonable eval size\n",
    "complaint_scenarios = random.sample(complaint_scenarios, min(15, len(complaint_scenarios)))\n",
    "\n",
    "# Wrap in correct input schema\n",
    "data = []\n",
    "for complaint in complaint_scenarios:\n",
    "    data.append({\n",
    "        \"inputs\": {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complaint\n",
    "            }]\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(data)} evaluation scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9l6zrp5n03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple scorers and run evaluation\n",
    "\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "import mlflow\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "project_directory = os.path.dirname(notebook_path)\n",
    "sys.path.append(project_directory)\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "# Multiple scorers to evaluate different aspects\n",
    "refund_reason = Guidelines(\n",
    "    name=\"refund_reason\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\",\n",
    "        \"Do not offer timing-related refunds for food quality or missing item complaints\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "decision_quality = Guidelines(\n",
    "    name=\"decision_quality\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Service complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "evidence_usage = Guidelines(\n",
    "    name=\"evidence_usage\",\n",
    "    guidelines=[\n",
    "        \"Decisions must be based on actual order data from tool calls\",\n",
    "        \"Credit amounts should be justified by delivery time comparisons to percentiles\",\n",
    "        \"Do not make assumptions without checking order details\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ResponsesAgent predict function wrapper for evaluation\n",
    "def predict_fn(messages):\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    request = ResponsesAgentRequest(input=messages)\n",
    "    response = AGENT.predict(request)\n",
    "    # Extract text from first output item\n",
    "    return response.output[0][\"text\"]\n",
    "\n",
    "# Run evaluation with multiple scorers\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=data,\n",
    "    scorers=[refund_reason, decision_quality, evidence_usage],\n",
    "    predict_fn=predict_fn\n",
    ")\n",
    "\n",
    "print(f\"Evaluation complete. Check MLflow UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttse3kj3pcj",
   "metadata": {},
   "source": [
    "#### log complaint agent to `UC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y8lfco9zzn",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "UC_MODEL_NAME = f\"{CATALOG}.ai.complaint_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "me3m6ovfqkd",
   "metadata": {},
   "source": [
    "#### deploy the agent to model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exckljo2zx4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME, \n",
    "    model_version=uc_registered_model_info.version, \n",
    "    scale_to_zero=False,\n",
    "    endpoint_name=f\"{dbutils.widgets.get('COMPLAINT_AGENT_ENDPOINT_NAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1syfkwcivl",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4i9yj8vjs2",
   "metadata": {},
   "source": [
    "##### record model in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ihnhnv5plw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add to UC-state\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from uc_state import add\n",
    "\n",
    "add(dbutils.widgets.get(\"CATALOG\"), \"endpoints\", deployment_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhjj7zuan9g",
   "metadata": {},
   "source": [
    "#### production monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1j0a0y21ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, ScorerSamplingConfig\n",
    "\n",
    "# Register scorers for production monitoring (10% sampling)\n",
    "decision_quality_monitor = Guidelines(\n",
    "    name=\"decision_quality_prod\",\n",
    "    guidelines=[\n",
    "        \"Food quality complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Missing item complaints should be classified as 'investigate', not 'auto_credit'\",\n",
    "        \"Legal threats or serious health concerns should be classified as 'escalate'\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_decision_quality\")\n",
    "\n",
    "refund_reason_monitor = Guidelines(\n",
    "    name=\"refund_reason_prod\",\n",
    "    guidelines=[\n",
    "        \"If a refund is offered, it must clearly relate to the complaint made by the user\"\n",
    "    ]\n",
    ").register(name=f\"{UC_MODEL_NAME}_refund_reason\")\n",
    "\n",
    "# Start monitoring with 10% sampling of production traffic\n",
    "decision_quality_monitor = decision_quality_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "refund_reason_monitor = refund_reason_monitor.start(\n",
    "    sampling_config=ScorerSamplingConfig(sample_rate=0.1)\n",
    ")\n",
    "\n",
    "print(\"✅ Production monitoring enabled with 10% sampling\")\n",
    "print(f\"   - decision_quality scorer monitoring: {decision_quality_monitor}\")\n",
    "print(f\"   - refund_reason scorer monitoring: {refund_reason_monitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}