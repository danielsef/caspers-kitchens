{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201d7b57-5c79-4436-905e-a4f350a032cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install osmnx==1.7.1 networkx==3.2.1 geopy==2.4.1 shapely==2.0.3 databricks-sdk[openai]>=0.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b2fa429-b8bc-4fbc-9d7c-a258ae9b71e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    sim_cfg_json = dbutils.widgets.get(\"sim_cfg_json\")\n",
    "except:\n",
    "    print(\"no widget\")\n",
    "    sim_cfg_json = ''\n",
    "    \n",
    "SIM_CFG = json.loads(sim_cfg_json) if sim_cfg_json != '' else json.load(open('./config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256edeef-7673-4be4-ae1b-a59e5ca4df85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  Ghost-Kitchen Event Simulator 2.1  →  Delta table                    ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "#  All tunables live in SIM_CFG  – supply as a JSON widget or edit below\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "import asyncio, datetime as dt, json, math, pickle, random, time, uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nest_asyncio, networkx as nx, numpy as np, osmnx as ox, pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 0.  CONFIGURATION                                                      │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "CFG = SIM_CFG; get = lambda k: CFG[k]\n",
    "RAND = random.Random(get(\"random_seed\")); np.random.seed(get(\"random_seed\"))\n",
    "\n",
    "CATALOG, SCHEMA, VOLUME = get(\"catalog\"), get(\"schema\"), get(\"volume\")\n",
    "START_TS, END_TS       = map(lambda t: dt.datetime.strptime(t,\"%Y-%m-%d %H:%M:%S\"),\n",
    "                             (get(\"start_ts\"), get(\"end_ts\")))\n",
    "SPEED_UP               = get(\"speed_up\")\n",
    "GK_ADDRESS, GK_R_MI    = get(\"gk_location\"), get(\"radius_mi\")\n",
    "LOC_NAME               = get(\"location_name\")\n",
    "GK_DRIVER_MPH          = get(\"driver_mph\")\n",
    "NOISE, SVC             = get(\"noise_pct\")/100, get(\"svc\")\n",
    "BATCH_ROWS, BATCH_SEC  = get(\"batch_rows\"), get(\"batch_seconds\")\n",
    "PING_SEC               = get(\"ping_sec\")\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/\"\n",
    "VOLUME_DIR  = Path(VOLUME_PATH).expanduser()   \n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 1.  ROAD GRAPH + NODES                                     │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ─── NEW imports (all come with osmnx’s deps) ────────────────────────────\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ─── Bulk-address cache (lazy-loaded once per run) ───────────────────────\n",
    "_BUILDINGS = None  # GeoDataFrame cache\n",
    "\n",
    "def load_buildings() -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Grab all building polygons that already have addr:housenumber + addr:street\n",
    "    tags inside the simulator’s radius.  No external HTTP calls after this.\n",
    "    \"\"\"\n",
    "    global _BUILDINGS\n",
    "    if _BUILDINGS is not None:           # reuse if we already fetched\n",
    "        return _BUILDINGS\n",
    "\n",
    "    center_pt = ox.geocoder.geocode(GK_ADDRESS)\n",
    "    tags = {\"addr:housenumber\": True, \"addr:street\": True}\n",
    "\n",
    "    # One Overpass query → GeoDataFrame of footprints with address tags\n",
    "    bldgs = ox.geometries_from_point(center_pt,\n",
    "                                     dist=GK_R_MI * 1609.34,\n",
    "                                     tags=tags)\n",
    "\n",
    "    # Keep only rows that really have both tags\n",
    "    bldgs = bldgs.dropna(subset=[\"addr:housenumber\", \"addr:street\"])\n",
    "    # footprints are Polygons; centroids are fine for nearest-search\n",
    "    bldgs = bldgs.to_crs(\"EPSG:4326\")  # ensure same CRS as nodes\n",
    "    _BUILDINGS = bldgs[[\"addr:housenumber\", \"addr:street\", \"geometry\"]]\n",
    "    return _BUILDINGS\n",
    "\n",
    "# ─── Replacement for load_nodes() ───────────────────────────────────────\n",
    "def load_nodes(g: nx.MultiDiGraph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return DataFrame with real lat/ lon and best available street address.\n",
    "    Works offline after the initial Overpass pull inside load_buildings().\n",
    "    \"\"\"\n",
    "    # ---- 1. Nodes → GeoDataFrame ---------------------------------------\n",
    "    node_rows = []\n",
    "    for nid, data in g.nodes(data=True):\n",
    "        lat, lon = data[\"y\"], data[\"x\"]\n",
    "        node_rows.append({\"node_id\": nid,\n",
    "                          \"lat\": lat,\n",
    "                          \"lon\": lon,\n",
    "                          \"geometry\": Point(lon, lat)})\n",
    "    gdf_nodes = gpd.GeoDataFrame(node_rows, crs=\"EPSG:4326\")\n",
    "\n",
    "    # ---- 2. Spatial join: nearest addressed footprint ------------------\n",
    "    bldgs = load_buildings()                       # cached after first call\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        gdf_nodes, bldgs,\n",
    "        how=\"left\",\n",
    "        distance_col=\"addr_dist_m\"                 # keep for curiosity\n",
    "    )\n",
    "\n",
    "    # ---- 3. Build final rows with fallback for empties -----------------\n",
    "    rows = []\n",
    "    for _, r in joined.iterrows():\n",
    "        if pd.notna(r[\"addr:housenumber\"]) and pd.notna(r[\"addr:street\"]):\n",
    "            label = f\"{r['addr:housenumber']} {r['addr:street']}\"\n",
    "        else:\n",
    "            label = f\"{RAND.randint(1, 9999)} Main St\"\n",
    "        rows.append(dict(node_id=int(r[\"node_id\"]),\n",
    "                         lat=r[\"lat\"],\n",
    "                         lon=r[\"lon\"],\n",
    "                         addr=label))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def load_graph() -> nx.MultiDiGraph:\n",
    "    ox.settings.log_console = False\n",
    "    g = ox.graph_from_point(ox.geocoder.geocode(GK_ADDRESS),\n",
    "                            dist=GK_R_MI*1609.34, network_type=\"drive\")\n",
    "    return g\n",
    "\n",
    "GRAPH = load_graph(); NODES = load_nodes(GRAPH)\n",
    "GK_LAT, GK_LON = ox.geocoder.geocode(GK_ADDRESS)\n",
    "GK_NODE = ox.distance.nearest_nodes(GRAPH, GK_LON, GK_LAT)\n",
    "comp_map = {n:cid for cid,comp in enumerate(nx.connected_components(GRAPH.to_undirected())) for n in comp}\n",
    "NODES = NODES[NODES.node_id.map(comp_map)==comp_map[GK_NODE]].reset_index(drop=True)\n",
    "def rand_customer():\n",
    "    r = NODES.sample(1, random_state=RAND.randrange(2**32)).iloc[0]  # ✔\n",
    "    return int(r.node_id), r.lat, r.lon, r.addr\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 2.  MENU + BRAND MOMENTUM                                              │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "ITEMS_DF = spark.read.table(\"caspers.simulator.items\").toPandas()\n",
    "ITEMS_BY_BRAND = {bid: grp.to_dict(\"records\") for bid, grp in ITEMS_DF.groupby(\"brand_id\")}\n",
    "BRANDS = list(ITEMS_BY_BRAND); RAND.shuffle(BRANDS)\n",
    "\n",
    "bm = get(\"brand_momentum\"); cuts = np.cumsum([bm[\"improving\"], bm[\"flat\"]])*len(BRANDS)\n",
    "IMPR, FLAT, DECL = BRANDS[:int(cuts[0])], BRANDS[int(cuts[0]):int(cuts[1])], BRANDS[int(cuts[1]):]\n",
    "rates = get(\"momentum_rates\")\n",
    "\n",
    "def brand_weight(day, total, b):\n",
    "    if b in IMPR:  f = (1+rates[\"growth\"])**(day/30)\n",
    "    elif b in DECL: f = (1-rates[\"decline\"])**(day/30)\n",
    "    else:          f = 1.0\n",
    "    return f\n",
    "\n",
    "def rand_basket(day, total, p_single=0.7, max_brands=4, items_rng=(1,4)):\n",
    "    w = np.array([brand_weight(day,total,b) for b in BRANDS]); w = w/w.sum()\n",
    "    chosen = [RAND.choices(BRANDS, weights=w, k=1)[0]] if RAND.random()<p_single \\\n",
    "             else RAND.choices(BRANDS, weights=w, k=RAND.randint(2,max_brands))\n",
    "    items=[]\n",
    "    for b in chosen:\n",
    "        for itm in RAND.sample(ITEMS_BY_BRAND[b], RAND.randint(*items_rng)):\n",
    "            rec = itm.copy(); rec[\"qty\"] = RAND.randint(1,3); items.append(rec)\n",
    "    return items\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 3.  DEMAND SHAPE                                                       │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "def minute_weights():\n",
    "    w = np.ones(1440)\n",
    "    for s,e,m in [(\"11:00\",\"13:30\",3), (\"17:00\",\"20:00\",3.5)]:\n",
    "        s_dt,e_dt = [dt.datetime.strptime(x,\"%H:%M\") for x in (s,e)]\n",
    "        s_m,e_m = s_dt.hour*60+s_dt.minute, e_dt.hour*60+e_dt.minute; span=e_m-s_m\n",
    "        for mi in range(s_m,e_m):\n",
    "            x = (mi-s_m)/span; w[mi] += (m-1)*(math.sin(math.pi*x)**2)\n",
    "    return w\n",
    "MIN_W = minute_weights()\n",
    "\n",
    "def orders_today(d, total, date):\n",
    "    base = CFG[\"orders_day_1\"]+(CFG[\"orders_last\"]-CFG[\"orders_day_1\"])*(d/total)\n",
    "    wd = {\"mon\":1,\"tue\":1.05,\"wed\":1.08,\"thu\":1.10,\"fri\":1.25,\"sat\":1.35,\"sun\":1.15}[date.strftime(\"%a\").lower()]\n",
    "    return base*wd*RAND.uniform(1-NOISE,1+NOISE)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 4.  ROUTING                                                            │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "def shortest_route(lat,lon):\n",
    "    cust = ox.distance.nearest_nodes(GRAPH, lon, lat)\n",
    "    try:\n",
    "        path = nx.shortest_path(GRAPH, GK_NODE, cust, weight=\"length\"); g = GRAPH\n",
    "    except nx.NetworkXNoPath:\n",
    "        g = GRAPH.to_undirected(); path = nx.shortest_path(g, GK_NODE, cust, weight=\"length\")\n",
    "    coords = [(g.nodes[n][\"y\"], g.nodes[n][\"x\"]) for n in path]\n",
    "    dist = sum(min(d[\"length\"] for d in g[u][v].values()) for u,v in zip(path[:-1],path[1:]))\n",
    "    return coords, dist\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 5.  JSON WRITER (flat directory, no date partitions)\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "def write_event_json(row: Dict):\n",
    "    \"\"\"\n",
    "    Save each event in a single, flat directory:\n",
    "        {json_dir}/<ts>-<event_id>.json\n",
    "    \"\"\"\n",
    "    ts = dt.datetime.strptime(row[\"ts\"], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    fname = f\"{ts:%Y%m%d-%H%M%S.%f}-{row['event_id']}.json\"\n",
    "    (VOLUME_DIR / fname).write_text(json.dumps(row))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 5.  DELTA WRITER + DATA-QUALITY                                        │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "EVENT_Q = asyncio.PriorityQueue(); CNT = 0\n",
    "GK_ID = uuid.uuid4().hex; DQ = get(\"dq\")\n",
    "\n",
    "def maybe_corrupt(ev, payload):\n",
    "    dq = DQ.get(ev, {}); return {k:(None if RAND.random()<dq.get(k,0) else v) for k,v in payload.items()}\n",
    "\n",
    "def enqueue(ts, ev, oid, seq, payload):\n",
    "    global CNT; CNT += 1\n",
    "    EVENT_Q.put_nowait((ts.timestamp(), CNT, {\n",
    "        \"event_id\": uuid.uuid4().hex,\n",
    "        \"event_type\": ev,\n",
    "        \"ts\": ts.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
    "        \"gk_id\": GK_ID,\n",
    "        \"location\": LOC_NAME,\n",
    "        \"order_id\": oid,\n",
    "        \"sequence\": seq,\n",
    "        \"body\": json.dumps(maybe_corrupt(ev, payload))\n",
    "    }))\n",
    "\n",
    "def flush(rows):\n",
    "    if not rows: return\n",
    "    ts = dt.datetime.now()                               \n",
    "    fname = ts.strftime(\"%Y%m%d-%H%M%S.%f\") + \".json\"\n",
    "    (VOLUME_DIR / fname).write_text(json.dumps(rows))\n",
    "\n",
    "async def consumer():\n",
    "    buf,last=[],time.time()\n",
    "    while True:\n",
    "        _,_,row = await EVENT_Q.get(); buf.append(row)\n",
    "        if len(buf)>=BATCH_ROWS or (time.time()-last)>=BATCH_SEC:\n",
    "            flush(buf); buf.clear(); last=time.time()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 6.  ORDER LIFECYCLE (real-time)                                        │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "MICRO = lambda n: dt.timedelta(microseconds=n)\n",
    "def gauss(mu_sigma): return max(0.1, RAND.gauss(*mu_sigma))\n",
    "\n",
    "driver_cfg = get(\"driver_arrival\")\n",
    "def driver_arrival_time(created_at, t_ready, t_pick):\n",
    "    \"\"\"Return a dt between created_at and t_pick with prob mass after ready.\"\"\"\n",
    "    if RAND.random() < driver_cfg[\"after_ready_pct\"]:\n",
    "        base, span = t_ready, (t_pick - t_ready)\n",
    "    else:\n",
    "        base, span = created_at, (t_ready - created_at)\n",
    "    frac = np.random.beta(driver_cfg[\"alpha\"], driver_cfg[\"beta\"])\n",
    "    t_arr = base + span * frac\n",
    "    if t_arr >= t_pick:  # safety: keep strict order\n",
    "        t_arr = t_pick - MICRO(1)\n",
    "    return t_arr\n",
    "\n",
    "async def play_order(created_at, day, total):\n",
    "    oid=uuid.uuid4().hex; seq=0\n",
    "    _,lat,lon,addr = rand_customer(); items = rand_basket(day,total)\n",
    "    pts,dist = shortest_route(lat,lon); drive_min = dist/1609.34/GK_DRIVER_MPH*60\n",
    "\n",
    "    t_cs  = created_at + dt.timedelta(minutes=gauss(SVC[\"cs\"]))\n",
    "    t_sf  = t_cs + dt.timedelta(minutes=gauss(SVC[\"sf\"]))\n",
    "    t_fr  = t_sf + dt.timedelta(minutes=gauss(SVC[\"fr\"]))\n",
    "    t_ready = t_fr\n",
    "    t_pick  = t_ready + dt.timedelta(minutes=gauss(SVC[\"rp\"]))\n",
    "    t_drop  = t_pick  + dt.timedelta(minutes=drive_min)\n",
    "    t_arr   = driver_arrival_time(created_at, t_ready, t_pick)\n",
    "\n",
    "    enqueue(created_at+MICRO(seq),\"order_created\",oid,seq,\n",
    "            dict(customer_lat=lat,customer_lon=lon,customer_addr=addr,items=items)); seq+=1\n",
    "    enqueue(t_cs+MICRO(seq),\"gk_started\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_sf+MICRO(seq),\"gk_finished\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_ready+MICRO(seq),\"gk_ready\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_arr+MICRO(seq),\"driver_arrived\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_pick+MICRO(seq),\"driver_picked_up\",oid,seq,\n",
    "            dict(route_points=pts, eta_mins=round(drive_min,1))); seq+=1\n",
    "\n",
    "    hops=max(1,int(drive_min*60//PING_SEC))\n",
    "    for h in range(1,hops):\n",
    "        p=h/hops; lat_i,lon_i=pts[int(p*(len(pts)-1))]\n",
    "        enqueue(t_pick+dt.timedelta(seconds=h*PING_SEC)+MICRO(seq),\"driver_ping\",oid,seq,\n",
    "                dict(progress_pct=round(p*100,1),loc_lat=lat_i,loc_lon=lon_i)); seq+=1\n",
    "\n",
    "    enqueue(t_drop+MICRO(seq),\"delivered\",oid,seq,\n",
    "            dict(delivered_lat=lat,delivered_lon=lon))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 7.  BACK-FILL (single write)                                           │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "def build_rows(ts, lat, lon, addr, items, pts, dist, svc, day, total):\n",
    "    rows=[]; seq=0; oid=uuid.uuid4().hex\n",
    "    drive=dist/1609.34/GK_DRIVER_MPH*60\n",
    "    t_cs=ts+dt.timedelta(minutes=gauss(svc[\"cs\"]))\n",
    "    t_sf=t_cs+dt.timedelta(minutes=gauss(svc[\"sf\"]))\n",
    "    t_fr=t_sf+dt.timedelta(minutes=gauss(svc[\"fr\"]))\n",
    "    t_ready=t_fr\n",
    "    t_pick=t_ready+dt.timedelta(minutes=gauss(svc[\"rp\"]))\n",
    "    t_drop=t_pick+dt.timedelta(minutes=drive)\n",
    "    t_arr=driver_arrival_time(ts, t_ready, t_pick)\n",
    "\n",
    "    def add(t,ev,p):\n",
    "        nonlocal seq\n",
    "        rows.append({\n",
    "            \"event_id\":uuid.uuid4().hex,\"event_type\":ev,\n",
    "            \"ts\":t.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\"gk_id\":GK_ID,\n",
    "            \"location\": LOC_NAME,\n",
    "            \"order_id\":oid,\"sequence\":seq,\n",
    "            \"body\":json.dumps(maybe_corrupt(ev,p))\n",
    "        }); seq+=1\n",
    "\n",
    "    add(ts,\"order_created\",dict(customer_lat=lat,customer_lon=lon,customer_addr=addr,items=items))\n",
    "    add(t_cs,\"gk_started\",{}); add(t_sf,\"gk_finished\",{}); add(t_ready,\"gk_ready\",{})\n",
    "    add(t_arr,\"driver_arrived\",{}); add(t_pick,\"driver_picked_up\",dict(route_points=pts,eta_mins=round(drive,1)))\n",
    "    hops=max(1,int(drive*60//PING_SEC))\n",
    "    for h in range(1,hops):\n",
    "        p=h/hops; lat_i,lon_i=pts[int(p*(len(pts)-1))]\n",
    "        add(t_pick+dt.timedelta(seconds=h*PING_SEC),\"driver_ping\",\n",
    "            dict(progress_pct=round(p*100,1),loc_lat=lat_i,loc_lon=lon_i))\n",
    "    add(t_drop,\"delivered\",dict(delivered_lat=lat,delivered_lon=lon))\n",
    "    return rows\n",
    "\n",
    "def gen_backfill(start, now, total_days):\n",
    "    back=[]\n",
    "    for d in range(total_days+1):\n",
    "        date = start.date()+dt.timedelta(days=d)\n",
    "        mean = orders_today(d,total_days,date)\n",
    "        lam  = mean/MIN_W.sum()*MIN_W; midnight=dt.datetime.combine(date,dt.time.min)\n",
    "        for m,v in enumerate(lam):\n",
    "            for _ in range(np.random.poisson(v)):\n",
    "                ts=midnight+dt.timedelta(minutes=m,seconds=RAND.randint(0,59))\n",
    "                if ts>=now: continue\n",
    "                _,lat,lon,addr = rand_customer(); items=rand_basket(d,total_days)\n",
    "                pts,dist=shortest_route(lat,lon)\n",
    "                back.extend(build_rows(ts,lat,lon,addr,items,pts,dist,SVC,d,total_days))\n",
    "    return back\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# 8.  MAIN SCHEDULER                                                     │\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "async def schedule():\n",
    "    now = dt.datetime.utcnow()\n",
    "    total_days = (END_TS.date()-START_TS.date()).days\n",
    "\n",
    "    flush(gen_backfill(START_TS, now, total_days))   # back-fill\n",
    "    asyncio.create_task(consumer())                  # live consumer\n",
    "\n",
    "    async def later(ts, d):\n",
    "        await asyncio.sleep(max(0,(ts-now).total_seconds()/SPEED_UP))\n",
    "        await play_order(ts, d, total_days)\n",
    "\n",
    "    futures=[]\n",
    "    for d in range(total_days+1):\n",
    "        date = START_TS.date()+dt.timedelta(days=d)\n",
    "        mean = orders_today(d,total_days,date)\n",
    "        lam  = mean/MIN_W.sum()*MIN_W; midnight = dt.datetime.combine(date,dt.time.min)\n",
    "        for m,v in enumerate(lam):\n",
    "            for _ in range(np.random.poisson(v)):\n",
    "                ts=midnight+dt.timedelta(minutes=m,seconds=RAND.randint(0,59))\n",
    "                if ts>=now:\n",
    "                    futures.append(asyncio.create_task(later(ts,d)))\n",
    "    if futures: await asyncio.gather(*futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9fa0de7-be2c-437c-8060-baf66d26dd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"👻  GK-sim → {VOLUME_PATH}  (×{SPEED_UP})\")\n",
    "print(f\"Running with config: {CFG}\")\n",
    "await schedule()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
